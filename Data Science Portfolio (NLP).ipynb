{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CRlf-VjoOZ8O"
   },
   "source": [
    "# Part 3 - Text analysis\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tU8BnCXIOZ8T"
   },
   "source": [
    "# 3.a Computing PMI\n",
    "\n",
    "In this assessment you are tasked to discover strong associations between concepts in Airbnb reviews. The starter code we provide in this notebook is for orientation only. The below imports are enough to implement a valid answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x_BJYvjpOZ8U"
   },
   "source": [
    "### Imports, data loading and helper functions\n",
    "\n",
    "We first connect our google drive, import pandas, numpy and some useful nltk and collections modules, then load the dataframe and define a function for printing the current time, useful to log our progress in some of the tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "gather": {
     "logged": 1622658727615
    },
    "id": "0z_s4GpwOZ8U"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.tag import pos_tag\n",
    "import re\n",
    "from collections import defaultdict,Counter\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import os\n",
    "from collections import Counter\n",
    "import string\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "gather": {
     "logged": 1622658727712
    },
    "id": "VFP8c6HlPF_-",
    "outputId": "0fa313c5-497c-44f6-f747-4d7ebf651661"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/azureuser/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/azureuser/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/azureuser/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/azureuser/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# nltk imports, note that these outputs may be different if you are using colab or local jupyter notebooks\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize,sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "gather": {
     "logged": 1622658727877
    },
    "id": "9JOWJqE9Pq5V"
   },
   "outputs": [],
   "source": [
    "# load stopwords\n",
    "sw = set(stopwords.words('english'))\n",
    "wordnet_lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "gather": {
     "logged": 1622658732794
    },
    "id": "LVD9Q3AxOZ8V"
   },
   "outputs": [],
   "source": [
    "#load data \n",
    "df =  pd.read_csv('reviews.csv')\n",
    "# deal with empty reviews\n",
    "df.comments = df.comments.fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 195
    },
    "gather": {
     "logged": 1622658733094
    },
    "id": "pNgPCqMPOZ8V",
    "outputId": "dd74578a-59c0-45c0-9228-3fefd61ac153"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>listing_id</th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>reviewer_id</th>\n",
       "      <th>reviewer_name</th>\n",
       "      <th>comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2818</td>\n",
       "      <td>1191</td>\n",
       "      <td>2009-03-30</td>\n",
       "      <td>10952</td>\n",
       "      <td>Lam</td>\n",
       "      <td>Daniel is really cool. The place was nice and ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2818</td>\n",
       "      <td>1771</td>\n",
       "      <td>2009-04-24</td>\n",
       "      <td>12798</td>\n",
       "      <td>Alice</td>\n",
       "      <td>Daniel is the most amazing host! His place is ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2818</td>\n",
       "      <td>1989</td>\n",
       "      <td>2009-05-03</td>\n",
       "      <td>11869</td>\n",
       "      <td>Natalja</td>\n",
       "      <td>We had such a great time in Amsterdam. Daniel ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2818</td>\n",
       "      <td>2797</td>\n",
       "      <td>2009-05-18</td>\n",
       "      <td>14064</td>\n",
       "      <td>Enrique</td>\n",
       "      <td>Very professional operation. Room is very clea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2818</td>\n",
       "      <td>3151</td>\n",
       "      <td>2009-05-25</td>\n",
       "      <td>17977</td>\n",
       "      <td>Sherwin</td>\n",
       "      <td>Daniel is highly recommended.  He provided all...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   listing_id    id        date  reviewer_id reviewer_name  \\\n",
       "0        2818  1191  2009-03-30        10952           Lam   \n",
       "1        2818  1771  2009-04-24        12798         Alice   \n",
       "2        2818  1989  2009-05-03        11869       Natalja   \n",
       "3        2818  2797  2009-05-18        14064       Enrique   \n",
       "4        2818  3151  2009-05-25        17977       Sherwin   \n",
       "\n",
       "                                            comments  \n",
       "0  Daniel is really cool. The place was nice and ...  \n",
       "1  Daniel is the most amazing host! His place is ...  \n",
       "2  We had such a great time in Amsterdam. Daniel ...  \n",
       "3  Very professional operation. Room is very clea...  \n",
       "4  Daniel is highly recommended.  He provided all...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "gather": {
     "logged": 1622658733267
    },
    "id": "O_9leP4VOZ8W",
    "outputId": "010fcf4a-300c-4749-8cb8-04bed1fe68cb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(452143, 6)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fJfVvyXyPYS4"
   },
   "source": [
    "###  3.a1 - Process reviews\n",
    "\n",
    "What to implement: A `function process_reviews(df)` that will take as input the original dataframe and will return it with three additional columns: `tokenized`, `tagged` and `lower_tagged`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "gather": {
     "logged": 1622658733434
    }
   },
   "outputs": [],
   "source": [
    "def process_text(comment):\n",
    "    '''\n",
    "    Function to tokenize text applied to each row of the 'comments' column\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    comment: string\n",
    "        string of each row of the 'comments' column\n",
    "    \n",
    "        \n",
    "    Returns\n",
    "    ----------\n",
    "    tokens: string\n",
    "        tokenized string\n",
    "\n",
    "    '''\n",
    "    # Remove punctuation\n",
    "    comment = comment.translate(str.maketrans('', '', string.punctuation))\n",
    "    # Tokenizer\n",
    "    tokenized_comment = word_tokenize(comment)\n",
    "    # Remove stop words\n",
    "    tokens = [word for word in tokenized_comment if word not in sw]\n",
    "    # Lemmitization\n",
    "    tokens = [wordnet_lemmatizer.lemmatize(word) for word in tokens]\n",
    "    # Remove single alphabets\n",
    "    tokens = [word for word in tokens if len(word) > 1]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "gather": {
     "logged": 1622658733859
    },
    "id": "b7jF_XXsQYgK"
   },
   "outputs": [],
   "source": [
    "def process_reviews(df):\n",
    "  '''\n",
    "    Inputs the original review dataframe, processes the raw reviews and outputs the dataframe with three additional columns: \n",
    "    tokenized, tagged and lower_tagged\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df: pandas.DataFrame\n",
    "      Dataframe containing the Airbnb reviews\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    df: pandas.DataFrame\n",
    "        Returns the original dataframe with three additional columns:\n",
    "          tokenized - Column with all reviews tokenized\n",
    "          tagged - Column with Part-of-Speech (PoS) tagging\n",
    "          lower_tagged - Column with lower case tokenized with tagging\n",
    "    '''\n",
    "  # your code here\n",
    "  df['tokenized'] = df['comments'].apply(process_text)\n",
    "  # PoS tagging\n",
    "  df['tagged'] = df['tokenized'].apply(pos_tag)\n",
    "  # Lower case\n",
    "  df['tokenized'] = df['tokenized'].apply(lambda x: [word.lower() for word in x])\n",
    "  # Lower case tagging\n",
    "  df['lower_tagged'] = df['tokenized'].apply(pos_tag)\n",
    "  return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "gather": {
     "logged": 1622660386234
    },
    "id": "rGYB8gx5Qq-P"
   },
   "outputs": [],
   "source": [
    "df = process_reviews(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sUaH-yNlQRL9"
   },
   "source": [
    "### 3.a2 - Create a vocabulary\n",
    "\n",
    "What to implement: A function `get_vocab(df)` which takes as input the DataFrame generated in step 1.c, and returns two lists, one for the 1,000 most frequent center words (nouns) and one for the 1,000 most frequent context words (either verbs or adjectives). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "gather": {
     "logged": 1622660386464
    }
   },
   "outputs": [],
   "source": [
    "vocab_size = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "gather": {
     "logged": 1622660386630
    },
    "id": "sAg6VRwdQQmg"
   },
   "outputs": [],
   "source": [
    "def get_vocab(df):\n",
    "  '''\n",
    "  Takes the dataframe generated above and returns two lists: the 1000 most frequent nouns and the 1000 most frequent verbs or\n",
    "  adjectives\n",
    "\n",
    "  Parameters\n",
    "  ----------\n",
    "  df: pandas.DataFrame\n",
    "    Dataframe containing the Airbnb reviews and the additional columns\n",
    "\n",
    "  Returns\n",
    "  ----------\n",
    "  cent_vocab: List\n",
    "    list containing the 1000 most frequent nouns (center words)\n",
    "  \n",
    "  cont_vocab: List\n",
    "    list containing the 1000 most frequent verbs or adjectives (context words)\n",
    "  '''\n",
    "    \n",
    "  tagged_list = df['lower_tagged'].to_list()\n",
    "  tagged_list = sum(tagged_list, [])\n",
    "  # Extract nouns\n",
    "  cent_vocab = [word for word, tag in tagged_list if (tag[0]=='N')]\n",
    "  # Extract verbs and adjectives\n",
    "  cont_vocab = [word for word, tag in tagged_list if (tag[0]=='J' or tag[0]=='V')]\n",
    "  \n",
    "  # Most frequent nouns\n",
    "  occurence_count = Counter(cent_vocab)\n",
    "  res = occurence_count.most_common(vocab_size)\n",
    "  cent_vocab = [word for word, count in res]\n",
    "  \n",
    "  # Most frequent verbs and adjectives\n",
    "  occurence_count = Counter(cont_vocab)\n",
    "  res = occurence_count.most_common(vocab_size)\n",
    "  cont_vocab = [word for word, count in res]\n",
    "  \n",
    "  \n",
    "  return cent_vocab, cont_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "gather": {
     "logged": 1622733907661
    },
    "id": "F_R5l4IVSk9-"
   },
   "outputs": [],
   "source": [
    "cent_vocab, cont_vocab = get_vocab(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qkqRGdQ_RUMg"
   },
   "source": [
    "### 3.a3 Count co-occurrences between center and context words\n",
    "\n",
    "What to implement: A function `get_coocs(df, center_vocab, context_vocab)` which takes as input the DataFrame generated in step 1, and the lists generated in step 2 and returns a dictionary of dictionaries, of the form in the example above. It is up to you how you define context (full review? per sentence? a sliding window of fixed size?), and how to deal with exceptional cases (center words occurring more than once, center and context words being part of your vocabulary because they are frequent both as a noun and as a verb, etc). Use comments in your code to justify your approach. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "gather": {
     "logged": 1622734133079
    },
    "id": "ddnfCbQWRd5R"
   },
   "outputs": [],
   "source": [
    "def get_coocs(df, cent_vocab, cont_vocab):\n",
    "  '''\n",
    "  Takes the dataframe and lists generated above and returns dictionary of dictionary containing: for each center word and \n",
    "  its assoicated context words\n",
    "\n",
    "  Parameters\n",
    "  ----------\n",
    "  df: pandas.DataFrame\n",
    "    Dataframe containing the Airbnb reviews and the additional columns\n",
    "  \n",
    "  cent_vocab: List\n",
    "    list containing the 1000 most frequent nouns (center words)\n",
    "  \n",
    "  cont_vocab: List\n",
    "    list containing the 1000 most frequent verbs or adjectives (context words)\n",
    "\n",
    "  Returns\n",
    "  ----------\n",
    "  coocs: Dict\n",
    "  dictionary containing dictionaries of each center work with the assoicated context words\n",
    "  '''\n",
    "\n",
    "  token_sent_list = df['tokenized'].to_list()\n",
    "  # Create dictionary as mentioned in the document\n",
    "  coocs = {ii:Counter({jj:0 for jj in cont_vocab if jj!=ii}) for ii in cent_vocab}\n",
    "\n",
    "  k=5  # Window Size\n",
    "\n",
    "  for sen in token_sent_list:\n",
    "      used_cent_words = []      # If centre word occurs more than once\n",
    "      for ii in range(len(sen)):\n",
    "          if sen[ii] in coocs and sen[ii] not in used_cent_words:\n",
    "              used_cent_words.append(sen[ii])\n",
    "              if ii < k:  # check if word occurs below window range\n",
    "                  c = Counter(set(sen[0:ii+k+1]) & set(cont_vocab)) # get verbs and adjectives within window range\n",
    "                  del c[sen[ii]]\n",
    "                  coocs[sen[ii]] = coocs[sen[ii]] + c\n",
    "              elif ii > len(sen)-(k+1): # check if word occurs above window range\n",
    "                  c = Counter(set(sen[ii-k::]) & set(cont_vocab)) # get verbs and adjectives within window range\n",
    "                  del c[sen[ii]]\n",
    "                  coocs[sen[ii]] = coocs[sen[ii]] + c\n",
    "              else:\n",
    "                  c = Counter(set(sen[ii-k:ii+k+1]) & set(cont_vocab)) # get verbs and adjectives within window range\n",
    "                  del c[sen[ii]]\n",
    "                  coocs[sen[ii]] = coocs[sen[ii]] + c\n",
    "\n",
    "  coocs = {ii:dict(coocs[ii]) for ii in cent_vocab}  # final dictionary of cent_vocab and cont_vocab\n",
    "  return coocs  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "gather": {
     "logged": 1622736572019
    },
    "id": "iTT_TOkaSoXL"
   },
   "outputs": [],
   "source": [
    "coocs = get_coocs(df, cent_vocab, cont_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "be6mOXqMRlt-"
   },
   "source": [
    "### 3.a4 Convert co-occurrence dictionary to 1000x1000 dataframe\n",
    "What to implement: A function called `cooc_dict2df(cooc_dict)`, which takes as input the dictionary of dictionaries generated in step 3 and returns a DataFrame where each row corresponds to one center word, and each column corresponds to one context word, and cells are their corresponding co-occurrence value. Some (x,y) pairs will never co-occur, you should have a 0 value for those cases. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "gather": {
     "logged": 1622736738489
    },
    "id": "C6WuM5U7RsBJ"
   },
   "outputs": [],
   "source": [
    "def cooc_dict2df(coocs):\n",
    "    '''\n",
    "    Takes the coocs dictionary and converts it into a dataframe. Also adds in missing colomns or rows\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    coocs: Dict\n",
    "        Dictionary containing dictionaries of each center work with the assoicated context words\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    coocdf: pandas.DataFrame\n",
    "        Dataframe which is the coocs dictionary converted into a dataframe    \n",
    "    '''\n",
    "\n",
    "    coocdf = pd.DataFrame.from_dict(coocs, orient='index') # convert dataframe to dictioanry\n",
    "    missing_cols = set(cont_vocab) - set(list(coocdf.columns)) # check any missing column\n",
    "    missing_rows = set(cent_vocab) - set(list(coocdf.index)) # check any missing row\n",
    "    for col in missing_cols: # add missing column if any\n",
    "        coocdf[col] = 0\n",
    "    if len(missing_rows) > 0:  # add missing row if any\n",
    "        row = pd.Series([0]*vocab_size)\n",
    "        temp_df = pd.DataFrame(columns=coocdf.columns)\n",
    "        for row_ in missing_rows:\n",
    "            row_df = pd.DataFrame([row], index = [row_], columns=coocdf.columns)\n",
    "            temp_df = pd.concat([row_df, temp_df])\n",
    "        coocdf = pd.concat([coocdf, temp_df])\n",
    "    coocdf.fillna(0, inplace=True)  # fill empty values with 0\n",
    "    coocdf = coocdf.astype(int)\n",
    "\n",
    "    return coocdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "gather": {
     "logged": 1622736741914
    },
    "id": "cwAflxldSrbg"
   },
   "outputs": [],
   "source": [
    "coocdf = cooc_dict2df(coocs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3EWllWryR-QL"
   },
   "source": [
    "### 3.a5 Raw co-occurrences to PMI scores\n",
    "\n",
    "What to implement: A function `cooc2pmi(df)` that takes as input the DataFrame generated in step 4, and returns a new DataFrame with the same rows and columns, but with PMI scores instead of raw co-occurrence counts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "gather": {
     "logged": 1622736745695
    },
    "id": "frTTs7-eSFHv"
   },
   "outputs": [],
   "source": [
    "def cooc2pmi(df):\n",
    "    '''\n",
    "    Takes the coocdf dataframe and replaces the raw co-occurrences with the PMI scores\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df: pandas.DataFrame\n",
    "        Dataframe which is the coocs dictionary converted into a dataframe    \n",
    "    \n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    pmidf: pandas.DataFrame\n",
    "        Dataframe with the PMI scores\n",
    "\n",
    "    '''\n",
    "    arr = df.to_numpy()  # numpy array\n",
    "    \n",
    "    # p(y|x) probability\n",
    "    row_totals = arr.sum(axis=1).astype(float)\n",
    "    prob_cols_given_row = (arr.T / row_totals).T\n",
    "\n",
    "    # p(y) p(x) probability\n",
    "    col_totals = arr.sum(axis=0).astype(float)\n",
    "    prob_of_cols = col_totals / sum(col_totals)\n",
    "    \n",
    "    # PMI: log( p(y|x) / p(y) p(x) )\n",
    "    ratio = prob_cols_given_row / prob_of_cols\n",
    "    ratio[ratio==0] = 0.00001\n",
    "    _pmi = np.log(ratio)\n",
    "    _pmi[_pmi < 0] = 0\n",
    "\n",
    "    pmidf = pd.DataFrame(_pmi, columns=df.columns, index=df.index)\n",
    "\n",
    "    return pmidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "gather": {
     "logged": 1622736748084
    },
    "id": "AGftXjXRSuQw"
   },
   "outputs": [],
   "source": [
    "pmidf = cooc2pmi(coocdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zaLRvjRySOYB"
   },
   "source": [
    "### 3.a6 Retrieve top-k context words, given a center word\n",
    "\n",
    "What to implement: A function `topk(df, center_word, N=10)` that takes as input: (1) the DataFrame generated in step 5, (2) a `center_word` (a string like `‘towels’`), and (3) an optional named argument called `N` with default value of 10; and returns a list of `N` strings, in order of their PMI score with the `center_word`. You do not need to handle cases for which the word `center_word` is not found in `df`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "gather": {
     "logged": 1622736751742
    },
    "id": "NlKUP9SgSXlL"
   },
   "outputs": [],
   "source": [
    "def topk(df, center_word, N=10):\n",
    "    '''\n",
    "    Takes the pmidf dataframe and finds the top N context words for a given center word\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df: pandas.DataFrame\n",
    "        Dataframe containing the PMI scores\n",
    "\n",
    "    center_word: string\n",
    "        string of a chosen noun, e.g 'location'\n",
    "    \n",
    "    N: int\n",
    "        Used to specify the number of context words to find.\n",
    "        Default = 10\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    list\n",
    "        returns a list if N strings, in descending order of their PMI score with the center word\n",
    "\n",
    "    '''\n",
    "    try:\n",
    "        values = df.loc[center_word, :].sort_values(ascending=False) # sort values according to PMI score\n",
    "        words = values.index\n",
    "        words = words[:N] # get top N words\n",
    "        top_words = []\n",
    "        for w in words:\n",
    "            top_words.append(w)\n",
    "            top_words\n",
    "        return top_words\n",
    "    except Exception as e:\n",
    "        return \"Center Word does not exist.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "gather": {
     "logged": 1622736754155
    },
    "id": "1I038zG1Sw62"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tea',\n",
       " 'kettle',\n",
       " 'nespresso',\n",
       " 'microwave',\n",
       " 'complimentary',\n",
       " 'fridge',\n",
       " 'bread',\n",
       " 'snack',\n",
       " 'dish',\n",
       " 'cheese']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topk(pmidf, 'coffee')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true,
    "gather": {
     "logged": 1622745281516
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 1000)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coocdf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true,
    "gather": {
     "logged": 1622745292646
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 1000)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pmidf.shape"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Part 3.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "microsoft": {
   "host": {
    "AzureML": {
     "notebookHasBeenCompleted": true
    }
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
